{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extra-vietnamese",
   "metadata": {},
   "source": [
    "# VIDEO2AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "smart-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "from moviepy.editor import VideoFileClip\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def video2audio(name):\n",
    "    file = 'video/'+name+'.mp4'\n",
    "    p = subprocess.call('ffmpeg -i '+file+' -q:a 0 -map a video/'+name+'.wav -y', shell = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-black",
   "metadata": {},
   "source": [
    "# AUDIO2TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occupational-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def audio2text(name):\n",
    "    english_text = ''\n",
    "    r = sr.Recognizer()\n",
    "    harvard = sr.AudioFile('video/'+name+'.wav')\n",
    "    shift  = 35\n",
    "    myduration = shift\n",
    "    myoffset = 0\n",
    "    try:\n",
    "        while 1:\n",
    "            with harvard as source:\n",
    "                r.adjust_for_ambient_noise(source)\n",
    "                audio = r.record(source, duration = myduration, offset = myoffset)\n",
    "                myoffset += shift\n",
    "                while 1:\n",
    "                    qq = r.recognize_google(audio, language=\"en-US\")\n",
    "                    try:\n",
    "                        if len(qq) != '':\n",
    "                            break\n",
    "                        else:\n",
    "                            qq=''\n",
    "                    except Exception as e: print(e) \n",
    "                english_text += qq\n",
    "                english_text += ' '\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    return english_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-integrity",
   "metadata": {},
   "source": [
    "# TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "organized-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import subprocess\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "from google_images_search import GoogleImagesSearch\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google_trans_new import google_translator\n",
    "from rutermextract import TermExtractor\n",
    "from spacy import displacy\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import math\n",
    "import subprocess\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "import jellyfish\n",
    "import datetime\n",
    "import os\n",
    "from collections import Counter\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import jellyfish\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "coronavirus = ['coronavirus', 'corona', 'virus', 'pandemic', 'covid19', 'moderna']\n",
    "dictionary=[]\n",
    "dictionary.append(coronavirus)\n",
    "\n",
    "def init_corpus():\n",
    "\n",
    "    corpus_book = ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',\n",
    "    'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt',\n",
    "    'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt',\n",
    "    'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt',\n",
    "    'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt',\n",
    "    'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
    "    ttext = ''\n",
    "\n",
    "    for j in corpus_book:\n",
    "        emma = nltk.corpus.gutenberg.words(j)\n",
    "\n",
    "        for i in emma:\n",
    "            ttext +=i\n",
    "            ttext +=' '\n",
    "    database = []\n",
    "    mass = ''\n",
    "    counter = 0\n",
    "    for jj in ttext:\n",
    "        if jj == '.':\n",
    "            if counter == 50:\n",
    "                database.append(mass)\n",
    "                counter = 0\n",
    "                mass = ''\n",
    "            else:\n",
    "                counter += 1\n",
    "        mass += jj\n",
    "    return database\n",
    "\n",
    "database = init_corpus()\n",
    "nlp = en_core_web_lg.load()\n",
    "translator = google_translator()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n",
    "    return lemmatizer.lemmatize(word, pos)\n",
    "\n",
    "def compute_tfidf(corpus):\n",
    "    def compute_tf(english_text):\n",
    "        tf_text = Counter(english_text)\n",
    "        for i in tf_text:\n",
    "            tf_text[i] = tf_text[i]/float(len(english_text))\n",
    "        return tf_text\n",
    "    def compute_idf(word, corpus):\n",
    "        return math.log10(len(corpus)/sum([1.0 for i in corpus if word in i]))\n",
    "    documents_list = []\n",
    "    for english_text in corpus:\n",
    "        tf_idf_dictionary = {}\n",
    "        computed_tf = compute_tf(english_text)\n",
    "    for word in computed_tf:\n",
    "        tf_idf_dictionary[word] = computed_tf[word] * compute_idf(word, corpus)\n",
    "        documents_list.append(tf_idf_dictionary)\n",
    "    return documents_list\n",
    "\n",
    "def freq(str):\n",
    "    freq_dict = {}\n",
    "    str_list = str.split()\n",
    "    unique_words = set(str_list)\n",
    "    for words in unique_words :\n",
    "        freq_dict.update({words: str_list.count(words)})\n",
    "    freq_dict = list(freq_dict.items())\n",
    "    freq_dict.sort(key=lambda i: i[1])\n",
    "    freq_dict.reverse()\n",
    "    return freq_dict\n",
    "\n",
    "def spacy_tags(english_text):\n",
    "    doc = nlp(english_text)\n",
    "    delimiter2 = 0\n",
    "    while True:\n",
    "        HighPrior = []\n",
    "        LowPrior = []\n",
    "        delimiter2 += 1\n",
    "        try:\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ !='DATE' and ent.label_ !='NORP' and ent.label_ !='ORDINAL' and ent.label_ !='CARDINAL' and ent.label_ !='TIME' and ent.label_ !='QUANTITY':\n",
    "                    HighPrior.append(lemmatizer.lemmatize(ent.text.lower()))\n",
    "                else:\n",
    "                    LowPrior.append(lemmatizer.lemmatize(ent.text.lower()))\n",
    "        except Exception as e: print(e)\n",
    "        if delimiter2 == 10 or len(HighPrior) != 0:\n",
    "            break\n",
    "\n",
    "    tags1type_spacy = list(set(HighPrior))\n",
    "    tags1type_spacy = [lemmatizer.lemmatize(j) for j in tags1type_spacy]\n",
    "\n",
    "    shift = 0\n",
    "    for i in range(0,len(tags1type_spacy)):\n",
    "        for j in range(0,len(tags1type_spacy)):\n",
    "            try:\n",
    "                if (str(tags1type_spacy[i+shift]).lower() in  str(tags1type_spacy[j]).lower() or str(tags1type_spacy[j]).lower() in str(tags1type_spacy[i+shift]).lower() or jellyfish.jaro_distance(str(tags1type_spacy[i+shift]).lower(), str(tags1type_spacy[j]).lower()) > 0.75 ) and jellyfish.jaro_distance(str(tags1type_spacy[i+shift]).lower(), str(tags1type_spacy[j]).lower()) != 1:\n",
    "                    if len(tags1type_spacy[j]) > len(tags1type_spacy[i+shift]):\n",
    "                        tags1type_spacy.remove(tags1type_spacy[i+shift])\n",
    "                    else:\n",
    "                        tags1type_spacy.remove(tags1type_spacy[j])\n",
    "            except: pass\n",
    "        shift+=1\n",
    "    tags2type_spacy = list(set(LowPrior))\n",
    "    tags2type_spacy = [lemmatizer.lemmatize(j) for j in tags2type_spacy]\n",
    "    shift = 0\n",
    "    for i in range(0,len(tags2type_spacy)):\n",
    "        for j in range(0,len(tags2type_spacy)):\n",
    "            try:\n",
    "                if (str(tags2type_spacy[i+shift]).lower() in  str(tags2type_spacy[j]).lower() or str(tags2type_spacy[j]).lower() in str(tags2type_spacy[i+shift]).lower() or jellyfish.jaro_distance(str(tags2type_spacy[i+shift]).lower(), str(tags2type_spacy[j]).lower()) > 0.75 ) and jellyfish.jaro_distance(str(tags2type_spacy[i+shift]).lower(), str(tags2type_spacy[j]).lower()) != 1:\n",
    "                    if len(tags2type_spacy[j]) > len(tags2type_spacy[i+shift]):\n",
    "                        tags2type_spacy.remove(tags2type_spacy[i+shift])\n",
    "                    else:\n",
    "                        tags2type_spacy.remove(tags2type_spacy[j])\n",
    "            except: pass\n",
    "        shift+=1\n",
    "    return tags1type_spacy, tags2type_spacy\n",
    "\n",
    "def nltk_tags(english_text):\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    result_count = []\n",
    "    term_extractor = TermExtractor()\n",
    "    translator = google_translator()\n",
    "    term_extractor = TermExtractor()\n",
    "\n",
    "    for term in term_extractor(english_text):\n",
    "        if term.count >= 1:\n",
    "            mm = term.normalized\n",
    "            result_count.append(mm)\n",
    "    return result_count\n",
    "\n",
    "def tf_idm_tags(english_text, database):\n",
    "    length = 0\n",
    "    for l in database:\n",
    "        length+=len(l)\n",
    "\n",
    "    corpus = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for text_block in database:\n",
    "        data = []\n",
    "        for jj in text_block.lower().replace('.','').replace(',','').replace('-','').split(' '):\n",
    "            data.append(compare_stemmer_and_lemmatizer(PorterStemmer(), WordNetLemmatizer(), word = lemmatizer.lemmatize(jj), pos = wordnet.VERB))\n",
    "        without_stop_words = [word for word in data if not word in stop_words]\n",
    "        corpus.append(without_stop_words)\n",
    "    length = 0\n",
    "    for l in corpus:\n",
    "        length+=len(l)\n",
    "\n",
    "    vocabulary = list(compute_tfidf(corpus)[0].items())\n",
    "    tf_idm = sorted(vocabulary, key=lambda tup: tup[1], reverse = True)\n",
    "    ans = []\n",
    "    [ans.append(j[0].lower().replace('\"','').replace(\"'\",'').replace('.','').replace(',','').replace('-','')) for j in tf_idm[0:5]]\n",
    "\n",
    "    return (ans)\n",
    "\n",
    "def get_tags(english_text):\n",
    "    \n",
    "    spacy1 = []\n",
    "    spacy2 = []\n",
    "    for i in english_text.split('. '):\n",
    "        [spacy1.append(j) for j in spacy_tags(i)[0]]\n",
    "        [spacy2.append(k) for k in spacy_tags(i)[1]]\n",
    "    tags=[]\n",
    "    blob = TextBlob(english_text)\n",
    "\n",
    "    tags = [i for i in list(blob.noun_phrases)]\n",
    "    tags = list(set(list(tags)))\n",
    "\n",
    "    delete = []\n",
    "\n",
    "    for j in tags:\n",
    "        for k in tags:\n",
    "            if jellyfish.jaro_distance(j, k) > 0.75 and jellyfish.jaro_distance(j, k)<0.99:\n",
    "                if len(j)>len(k):\n",
    "                    delete.append(j)\n",
    "                else:\n",
    "                    delete.append(k)\n",
    "                if j in k and k!=j:\n",
    "                    delete.append(j)\n",
    "                if k in j and k!=j:\n",
    "                    delete.append(k)\n",
    "    for kk in delete:\n",
    "        try:\n",
    "            tags.remove(kk)\n",
    "        except:pass\n",
    "    [tags.append(i) for i in spacy1]\n",
    "    tags = sorted(list(set(tags)))\n",
    "\n",
    "    for i in tags:\n",
    "        if len(i)<=1:\n",
    "            tags.remove(i)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for i in list(stop_words):\n",
    "        if i in tags:\n",
    "            tags.remove(i)\n",
    "            \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for i in list(stop_words):\n",
    "        if i in tags:\n",
    "            tags.remove(i)\n",
    "\n",
    "    tags2 = [j for j in tags]\n",
    "    [tags2.append(i) for i in spacy2]\n",
    "    tags2 = sorted(list(set(tags2)))\n",
    "    d = {}\n",
    "    for i in tags:\n",
    "        if english_text.lower().find(i) != -1:\n",
    "            d[english_text.lower().find(i)]=i\n",
    "    tags = list(dict(sorted(d.items())).values())\n",
    "    d = {}\n",
    "    for i in tags2:\n",
    "        if english_text.lower().find(i) != -1:\n",
    "            d[english_text.lower().find(i)]=i\n",
    "    tags2 = list(dict(sorted(d.items())).values())\n",
    "                                                                   \n",
    "    delete = []\n",
    "    for j in tags:\n",
    "        for k in tags:\n",
    "            if j in k and k!=j:\n",
    "                delete.append(j)\n",
    "            elif k in j and k!=j:\n",
    "                delete.append(k)\n",
    "        for kk in delete:\n",
    "            try:\n",
    "                tags.remove(kk)\n",
    "            except:pass\n",
    "    delete = []\n",
    "    for j in tags2:\n",
    "        for k in tags2:\n",
    "            if j in k and k!=j:\n",
    "                delete.append(j)\n",
    "            elif k in j and k!=j:\n",
    "                delete.append(k)\n",
    "    for kk in delete:\n",
    "        try:\n",
    "            tags2.remove(kk)\n",
    "        except:pass\n",
    "    return tags,tags2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-basics",
   "metadata": {},
   "source": [
    "# embeddings_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "injured-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import itertools\n",
    "import spacy\n",
    "\n",
    "def clean_text(sentence):\n",
    "    clean_sentence = \"\".join(l for l in sentence if l not in string.punctuation)\n",
    "    return clean_sentence\n",
    "\n",
    "def cosine_similarity_calc(vec_1,vec_2):\n",
    "    sim = np.dot(vec_1,vec_2)/(np.linalg.norm(vec_1)*np.linalg.norm(vec_2))\n",
    "    return sim\n",
    "\n",
    "def embeddings_similarity(sentences):\n",
    "    sentence_pairs = list(itertools.combinations(sentences, 2))\n",
    "    sentence_a = [pair[0] for pair in sentence_pairs]\n",
    "    sentence_b = [pair[1] for pair in sentence_pairs]\n",
    "    sentence_pairs_df = pd.DataFrame({'sentence_a':sentence_a, 'sentence_b':sentence_b})\n",
    "    sentence_pairs_df = sentence_pairs_df.loc[pd.DataFrame(np.sort(sentence_pairs_df[['sentence_a', 'sentence_b']],1),index=sentence_pairs_df.index).drop_duplicates(keep='first').index]\n",
    "    sentence_pairs_df = sentence_pairs_df[sentence_pairs_df['sentence_a'] != sentence_pairs_df['sentence_b']]\n",
    "    embeddings = spacy.load('en_core_web_lg')\n",
    "    sentence_pairs_df['similarity'] = sentence_pairs_df.apply(lambda row: cosine_similarity_calc(embeddings(clean_text(row['sentence_a'])).vector,\n",
    "    embeddings(clean_text(row['sentence_b'])).vector), axis=1)\n",
    "    return sentence_pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-margin",
   "metadata": {},
   "source": [
    "# Main DF creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fuzzy-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('video\\df.xlsx')\n",
    "df['text'] = ''\n",
    "df['tags'] = ''\n",
    "df['metrics'] = ''\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    name = df.file[i].split('.')[0]\n",
    "    video2audio(name)\n",
    "    #df.text[i] = audio2text(name)\n",
    "    #df.tags[i] = ', '.join(get_tags(df.text[i])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-sudan",
   "metadata": {},
   "source": [
    "# Find recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "compact-glossary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['europe, wild hamsters, grasslands tv, court desert, calories hamster cheeks, body weight, big cheeks, narrow bottlenecks', \"strictly daphne, mobility service, bridget, lily anthony's, 's story 4jobs, trio show, disabilities weather human rk900, brasted, joy, languages touch, body rubs, hymn, 'm feeling, way chicken, puppy training, foster, bots, dual bike, way shape, 's differences\"]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15672/4065731811.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mget_recommendation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15672/4065731811.py\u001b[0m in \u001b[0;36mget_recommendation\u001b[1;34m(name, df)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtags_to_needed_article\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15672/764553177.py\u001b[0m in \u001b[0;36membeddings_similarity\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0msentence_pairs_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_pairs_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_pairs_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence_a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentence_b'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentence_pairs_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0msentence_pairs_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_pairs_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence_pairs_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence_a'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0msentence_pairs_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence_b'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_lg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     sentence_pairs_df['similarity'] = sentence_pairs_df.apply(lambda row: cosine_similarity_calc(embeddings(clean_text(row['sentence_a'])).vector,\n\u001b[0;32m     24\u001b[0m     embeddings(clean_text(row['sentence_b'])).vector), axis=1)\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, disable, exclude, config)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \"\"\"\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"blank:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# installed as package\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# path to model data directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \"\"\"\n\u001b[0;32m    354\u001b[0m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\en_core_web_lg\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m     return load_model_from_path(\n\u001b[0m\u001b[0;32m    515\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(self, path, exclude)\u001b[0m\n\u001b[0;32m   1849\u001b[0m             \u001b[1;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"vocab\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1851\u001b[1;33m         \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1852\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_link_components\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m   1172\u001b[0m         \u001b[1;31m# Split to support file names like meta.json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m             \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1175\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(p, proc)\u001b[0m\n\u001b[0;32m   1843\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"from_disk\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1844\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1845\u001b[1;33m             deserializers[name] = lambda p, proc=proc: proc.from_disk(\n\u001b[0m\u001b[0;32m   1846\u001b[0m                 \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vocab\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m             )\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(self, path, exclude)\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;34m\"patterns\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mload_patterns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         }\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m   1172\u001b[0m         \u001b[1;31m# Split to support file names like meta.json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m             \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1175\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py\u001b[0m in \u001b[0;36mload_patterns\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mload_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_patterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrsly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_msgpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         deserialize = {\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py\u001b[0m in \u001b[0;36madd_patterns\u001b[1;34m(self, patterns)\u001b[0m\n\u001b[0;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python38\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, patterns, attrs, index)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attrs_unnormed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m         \u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_token_attrs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "name = 'animals/2.mp4'\n",
    "\n",
    "def get_recommendation(name, df):\n",
    "    ind = df.index[df['file'] == name].tolist()[0]\n",
    "    tags_to_needed_article = df.tags[ind]\n",
    "    \n",
    "    for i in range(0,len(df)):\n",
    "        sentences = [tags_to_needed_article, df.tags[i]]\n",
    "        print(sentences)\n",
    "        df.metrics[i] = embeddings_similarity(sentences).similarity[0]\n",
    "        \n",
    "    return df\n",
    "\n",
    "get_recommendation(name, df1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
